<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> - Articles</title>
    <description>Machine Learning, NLP</description>
    <link>
    </link>
    
      
      <item>
        <title>ACL 2019 Highlights</title>
        
          <description>&lt;p&gt;This post discusses highlights of the main conference of the 2019 Annual Meeting of the Association for Computational Linguistics (ACL 2019). Note that these notes are written with business applications in mind.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 01 Aug 2019 16:03:36 -0500</pubDate>
        <link>
        /acl2019</link>
        <guid isPermaLink="true">/acl2019</guid>
      </item>
      
    
      
      <item>
        <title>mcQA - Multiple Choice Question Answering</title>
        
          <description>&lt;p&gt;mcQA is a multiple choice question answering python library, using Language Models.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 10 Jul 2019 02:25:36 -0500</pubDate>
        <link>
        /mcQA</link>
        <guid isPermaLink="true">/mcQA</guid>
      </item>
      
    
      
      <item>
        <title>Building Parallel Corpora Using Cross-Lingual BOW</title>
        
          <description>&lt;p&gt;Training machine translation models requires a huge amount of parallel data.
Consequently, there has been many works suggesting different methods to build
bilingual corpora, leading to the construction of reliable training datasets for
machine translation systems.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 13 Jul 2018 02:25:36 -0500</pubDate>
        <link>
        /Building-Parallel-Corpora-CLBOW</link>
        <guid isPermaLink="true">/Building-Parallel-Corpora-CLBOW</guid>
      </item>
      
    
      
      <item>
        <title>Clause Augmentation for Better NMT</title>
        
          <description>&lt;p&gt;Most public parallel corpora are formed of long sentences. Consequently, neural translation models tend to generate a long output with n-grams repetition, even when they are exposed to a short sequence or a one-word example. This causes the repetition problem, explained by the fact that none of the neurons learns the representation of length, thus the model generates a long sequence by default. In other terms, the probability of appearance of the end-of-sentence token &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;eos&amp;gt;&lt;/code&gt; will not be high enough to stop the output generation when translating a short sequences.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 01 Apr 2018 02:25:36 -0500</pubDate>
        <link>
        /Clause-Augmentation-for-Better-NMT</link>
        <guid isPermaLink="true">/Clause-Augmentation-for-Better-NMT</guid>
      </item>
      
    
  </channel>
</rss>
