<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>EMNLP 2018 Highlights</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Software Engineer, Machine Learning" />
    <link rel="shortcut icon" href="/assets/images/favicon.png?" type="image/png" />
    <link rel="canonical" href="/emnlp2018" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Taycir Yahmed" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="EMNLP 2018 Highlights" />
    <meta property="og:description" content="In this post, I share my notes from the conference on Empirical Methods for Natural Language Processing, which took place in Brussels, Belgium, from October 31th to November 4th 2018. The tutorials, workshops and collocated conferences took place on the first two days. The main conference took place from November" />
    <meta property="og:url" content="/emnlp2018" />
    <meta property="og:image" content="/assets/images/pano-tapis-de-fleurs-darkened-resize10.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2018-12-05T10:25:36+00:00" />
    <meta property="article:modified_time" content="2018-12-05T10:25:36+00:00" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="EMNLP 2018 Highlights" />
    <meta name="twitter:description" content="In this post, I share my notes from the conference on Empirical Methods for Natural Language Processing, which took place in Brussels, Belgium, from October 31th to November 4th 2018. The tutorials, workshops and collocated conferences took place on the first two days. The main conference took place from November" />
    <meta name="twitter:url" content="/" />
    <meta name="twitter:image" content="/assets/images/pano-tapis-de-fleurs-darkened-resize10.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Taycir Yahmed" />
    <meta name="twitter:site" content="@TaycirYahmed" />
    <meta name="twitter:creator" content="@TaycirYahmed" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Taycir Yahmed",
        "logo": "/"
    },
    "url": "/emnlp2018",
    "image": {
        "@type": "ImageObject",
        "url": "/assets/images/pano-tapis-de-fleurs-darkened-resize10.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/emnlp2018"
    },
    "description": "In this post, I share my notes from the conference on Empirical Methods for Natural Language Processing, which took place in Brussels, Belgium, from October 31th to November 4th 2018. The tutorials, workshops and collocated conferences took place on the first two days. The main conference took place from November"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="EMNLP 2018 Highlights" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Taycir Yahmed</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-arabic-nlp" role="menuitem"><a href="https://abjed.github.io/Arabic-NLP-resources/">Arabic NLP Resources</a></li>
    <li class="nav-arabic-terms" role="menuitem"><a href="https://abjed.github.io/Arabic-Scientific-Technical-Terms/">Arabic Scientific and Technical Terms</a></li>
    <!--<li class="nav-getting-started" role="menuitem"><a href="/tag/getting-started/">Getting Started</a></li>
    <li class="nav-try-ghost" role="menuitem"><a href="https://ghost.org">Try Ghost</a></li>-->
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
                <a class="social-link social-link-tw" href="https://twitter.com/TaycirYahmed" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            
            
                <a class="social-link social-link-tw" href="https://www.linkedin.com/in/taycir" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1"  width="24" height="24" viewBox="0 0 24 24"><path fill="#FFFFFF" d="M21,21H17V14.25C17,13.19 15.81,12.31 14.75,12.31C13.69,12.31 13,13.19 13,14.25V21H9V9H13V11C13.66,9.93 15.36,9.24 16.5,9.24C19,9.24 21,11.28 21,13.75V21M7,21H3V9H7V21M5,3A2,2 0 0,1 7,5A2,2 0 0,1 5,7A2,2 0 0,1 3,5A2,2 0 0,1 5,3Z" /></svg>
</a>
            
            
                <a class="social-link social-link-tw" href="https://github.com/tayciryahmed" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-github" viewBox="0 0 16 16">
  <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8"/>
</svg>
</a>
            
        </div>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full post ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime=" 5 December 2018"> 5 December 2018</time>
                    
                </section>
                <h1 class="post-full-title">EMNLP 2018 Highlights</h1>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/images/pano-tapis-de-fleurs-darkened-resize10.jpg)">
            </figure>
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <p>In this post, I share my notes from the conference on Empirical Methods for Natural Language Processing, which took place in Brussels, Belgium, from October 31<sup>th</sup> to November 4<sup>th</sup> 2018. The tutorials, workshops and collocated conferences took place on the first two days. The main conference took place from November 2<sup>nd</sup> to November 3<sup>rd </sup>2018.</p>

<h1 id="day-1-october-31st-2018">Day 1: October 31<sup>st</sup>, 2018</h1>

<h2 id="tutorial-1-joint-models-for-nlp">Tutorial 1: Joint models for NLP</h2>

<p>The tutorial was presented by Yue Zhang from Westlake Institute for Advanced Study. The presentation exposes the interest in training joint models for NLP, with two different strategies: statistical and deep learning methods. Concepts in the presentation are explained through examples and papers.</p>

<ul>
  <li>Motivation for joint models:
    <ul>
      <li>In NLP, many tasks are related (e.g. NER, chunking and POS tagging) or pipelined (e.g. tokenization and POS tagging).</li>
      <li>Joint models allow information exchange between tasks and error propagation reduction.</li>
    </ul>
  </li>
  <li>Statistical methods include transition based and graph based methods.</li>
  <li>Deep learning methods:
    <ul>
      <li>Transition-based models (Joint Learning / Joint Search): predict the next action to perform in order to get the right prediction.</li>
      <li>Graph-based models (Joint Learning / Separate search): they consist of multi-task learning methods: cross-task, cross-lingual, cross-domain and cross-standard.
        <ul>
          <li>Cross-tasks: Bear in mind that not all tasks are mutually beneficial.</li>
          <li>Cross-lingual: Standard (e.g. multilingual neural transliteration between morphological similar languages), Regularization (e.g. Low resource dependency parsing: transfer encoding parameters), staking (e.g. parameter sharing in Singlish parser), pre-training (e.g. Fine-tuning a pre-trained model on a low resource language) and adversarial training (e.g. cross lingual sequence labeling).</li>
          <li>Cross-domain: e.g. multi-domain sentiment classification.</li>
          <li>Cross-standard: e.g. output results corresponding to various treebanks.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="workshop-5-scai---search-oriented-conversational-ai">Workshop 5: SCAI - Search-Oriented Conversational AI</h2>

<p><a href="https://scai.info/">Link</a> to the workshop page.</p>

<h3 id="keynote-towards-natural-conversation-with-machines">Keynote: Towards natural conversation with machines</h3>

<p>A keynote presented by Milica Gašić from University of Cambridge, the Dialogue Systems Group.</p>

<ul>
  <li>Motivations: Popularity of virtual assistants (1 billion calls / day), expected revenue 16 billion USD in 2021 (Tractica), weakness of current models (unnatural, narrow domain).</li>
  <li>Challenges:
    <ul>
      <li>Immediate: Models that keep track of context cannot scale.</li>
      <li>Short-term: Operate only on predefined databases, manage only short conversations, user specific models are unrealistic, etc.</li>
      <li>Long-term: Modeling rich conversations, teaching a machine to talk about a piece of text, supporting large/infinite action sets.</li>
    </ul>
  </li>
  <li>The presentation shows how machine learning solves two challenges in dialogue systems:
    <ul>
      <li>Belief tracking: Tracking every concept requires training data for each one: Reuse knowledge for different concepts, use semantically-constrained word vector embeddings and architectures that share parameters.</li>
      <li>Response experience from a large set of possibilities, using deep reinforcement learning.</li>
    </ul>
  </li>
</ul>

<h3 id="papers-presentations">Papers presentations</h3>

<ul>
  <li>A Reinforcement Learning-driven Translation Model for Search-Oriented Conversational Systems.
    <ul>
      <li>Learning the mapping between natural language expressed needs and keywords expressed needs, using machine translation paradigms.</li>
      <li>Inject task objectives within the model using reinforcement learning methods.</li>
      <li>For more details, check out the <a href="https://arxiv.org/abs/1809.01495">paper</a>.</li>
    </ul>
  </li>
  <li>Research Challenges in Building a Voice-based Artificial Personal Shopper - Position Paper.
    <ul>
      <li>An artificial personal shopper is a voice based dialog system used to enrich online shopping, by replicating personal shopping agents in a brick and mortar store.</li>
      <li>This is a difficult task requiring effective knowledge and understanding: e.g. in order to correctly answer the question “Is the Bose headphone compatible with my phone?”, the agent has to know:
        <ul>
          <li>What type of phone the customer has / refers to?</li>
          <li>What is the model of the “Bose headphone”?</li>
          <li>Whether the headphone is compatible with the customer’s phone?</li>
        </ul>
      </li>
      <li>Types of data required: Product information, user information and consumer-generated content.</li>
      <li>Research challenges:
        <ol>
          <li>How to process a voice utterance? ASR is not perfect; we need a robust approach that provides a precise response for a noisy utterance.</li>
          <li>How to identify relevant response source(s) for a given utterance?
            <ul>
              <li>Identifying the relevant response source effectively, while minimizing the missing relevant sources for the product domain.</li>
              <li>Aggregating the results from various sources.</li>
            </ul>
          </li>
          <li>How to identify key phrases in a user’s utterance?
            <ul>
              <li>Key phrases in a query contribute significantly to the search results.</li>
              <li>Require an effective approach for identifying key phrases, the product domain and in the noisy voice transcription domain.</li>
            </ul>
          </li>
          <li>How to infer which product / entity the user refers to?
            <ul>
              <li>Personalized information must be taken into consideration.</li>
              <li>Incorporate coreference resolution and anaphora for personal shopper products / entities.</li>
            </ul>
          </li>
          <li>How to generate a natural language response?
            <ul>
              <li>Generating informative and conversational responses.</li>
              <li>Generating a multi-facet answer to a subjective question that represents multiple opinions.</li>
            </ul>
          </li>
          <li>How to evaluate an end-to-end personal shopper system?
            <ul>
              <li>Evaluation based on the criteria of both the relevance towards the user information needs and the replication of a humanlike conversation.</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>For more details, check out the <a href="http://www.aclweb.org/anthology/W18-5706">paper</a>.</li>
    </ul>
  </li>
</ul>

<h3 id="panel-discussion">Panel discussion</h3>

<p>The panel was animated by Milica Gašić (University of Cambridge), Antoine Bordes (Facebook AI Research), Jason Weston (Facebook) and Bill Dolan (Microsoft Research).</p>

<ul>
  <li>Future of conversational AI?
    <ul>
      <li>Jason Weston: Online learning.</li>
      <li>Antoine Bordes: End-to-end response specialized bot (task-oriented and domain-oriented), not general purpose chatbots but specialized on domains to insure complex tasks’ completion.</li>
      <li>Bill Dolan: Discussed the importance of chit-chat in task oriented chatbots.</li>
      <li>Milica Gašić: Bigger challenges for Conversational AI : Humans don’t talk to systems the way they talk to humans. Need to work on question generation, not only response generation.</li>
    </ul>
  </li>
  <li>How to evaluate the quality of dialogue systems?
    <ul>
      <li>If we use real users, it would be impossible to compare results across groups: Milica Gašić: Compare to statistical dialogue systems and / or use standard data sets.</li>
      <li>Bill Dolan: For response generation, BLEU still has an added value, despite its known weaknesses.</li>
      <li>For human bot evaluation: precision, recall, perplexity. BLEU is for MT but not for dialogue: We would need at least 100 references. Plus, for a more personalized experience, we need a more personalized evaluation metric.</li>
    </ul>
  </li>
  <li>Difficulty for conversational AI?
    <ul>
      <li>Learning in a compliant environment with encrypted data.</li>
      <li>Seq2seq architectures are suspicious; they come from MT (difficult linguistic task).</li>
      <li>To ensure fluency: combine other signals with linguistic signal / Importance of ML advancements to have better solutions, models, architectures.</li>
      <li>Dialogue incorporates a lot of linguistic tasks: go beyond text with gestures (like human conversation) and combine multimodal data.</li>
      <li>Algorithms are inadequate to work with small amounts of data; it is difficult to annotate and collect data frame by frame.</li>
      <li>Assess algorithms on computer vision and text. Is it possible to do it directly on speech / image instead of text?</li>
    </ul>
  </li>
  <li>Search versus Chatbots?
    <ul>
      <li>The user experience is formally different:
        <ul>
          <li>Chatbots should ensure full representation of belief / generate questions to clarify things.</li>
          <li>Search (over documents) incorporates external knowledge in dialog (Information retrieval challenges).</li>
          <li>Conversational search should be an integral part of the dialogue.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="keynote-understanding-the-user-in-social-bot-conversations">Keynote: Understanding the user in social bot conversations</h3>

<p>This presentation was delivered by Mari Ostendorf (University of Washington).</p>

<ul>
  <li>Winning solution of Amazon Alexa Prize: Building a bot that converses coherently and engagingly with people on popular topics and current events.</li>
  <li>Types of conversational AI systems:
    <ul>
      <li>Virtual assistant: execute commands, answer questions, limited social back and forth.</li>
      <li>Social bot:  2-way social and information exchange</li>
      <li>Chat bot: chitchat, limited content to talk about.</li>
    </ul>
  </li>
  <li>Importance of user modeling :
    <ul>
      <li>User interests vary</li>
      <li>For text-based search, personalized query completion improves over popular queries.</li>
    </ul>
  </li>
  <li>Differences in conversational AI Paradigms</li>
</ul>

<table>
  <tr>
   <td>
   </td>
   <td>
Speech/Language understanding
   </td>
   <td>Dialog management 
   </td>
   <td>Back-end application
   </td>
   <td>Response generation 
   </td>
  </tr>
  <tr>
   <td>Assistant 
   </td>
   <td>Task intents, form filling.
   </td>
   <td>Narrow options and execute tasks. Reward=Timely tasks completion. 
   </td>
   <td>Structured data bases. 
   </td>
   <td>Constrained domain.
   </td>
  </tr>
  <tr>
   <td>Social Bot
   </td>
   <td>Social and info intents.
   </td>
   <td>Learn about interests and make suggestions. Reward=user satisfaction. 
   </td>
   <td>Unstructured information. 
   </td>
   <td>Open domain.
   </td>
  </tr>
</table>

<ul>
  <li>Constrains:
    <ul>
      <li>Speech recognition is imperfect.</li>
      <li>No sentence segmentation or pause information.</li>
      <li>We cannot assume that two conversations coming from the same device correspond to the same user.</li>
    </ul>
  </li>
  <li>Design philosophy of the winning solution:
    <ul>
      <li>Content driven: daily content mining, large and dynamic content collection, etc.</li>
      <li>User centric: detecting user sentiments, personality, etc.</li>
    </ul>
  </li>
  <li>Language understanding and generation in the winning solution:
    <ul>
      <li>Language understanding: multidimensional utterance representation, different detectors for sentiments, intent, opinion, topic, etc.</li>
      <li>Generation: Inform AND ground.</li>
    </ul>
  </li>
  <li>Hierarchical dialog management :
    <ul>
      <li>Master (Global):
        <ul>
          <li>Rank topics, mini-tasks, content</li>
          <li>Consider: topic coherence, user engagement, content availability</li>
        </ul>
      </li>
      <li>Mini-tasks (Local):
        <ul>
          <li>Greetings, goodbye, menu.</li>
          <li>Probe user personality.</li>
          <li>Discuss a new article, movie.</li>
          <li>Tell a fact, thought, advice, joke, etc.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Content management in a social bot:
    <ul>
      <li>Crawl online content</li>
      <li>Filter inappropriate / depressing content.</li>
      <li>Index interesting and uplifting content.</li>
      <li>Knowledge graph building.</li>
    </ul>
  </li>
  <li>Evaluation of social bots: human + duration of conversations.</li>
  <li>Diagnostic evaluation:
    <ul>
      <li>User ratings are expensive, sparse and present a high variance.</li>
      <li>Define sub-dialog rewards using dialog state information (initialization, topic stack, etc.)</li>
    </ul>
  </li>
  <li>Content preferences vary with respect to the user’s personality and emotions.</li>
  <li>Different user goals: information seeking, opinion sharing, getting to know each other.</li>
</ul>

<h2 id="brussels-nlp-meetup">Brussels NLP meetup</h2>

<h3 id="understanding-structure-in-language-through-wikipedia-edits">Understanding Structure in Language through Wikipedia Edits</h3>

<p>This presentation, delivered by Manaal Faruqui (Google), explains the paper: WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse.</p>

<ul>
  <li>Main idea: using Wikipedia edits to solve downstream NLP problems, including splitting, replacing and rephrasing sentences.</li>
  <li>Two contributions:
    <ul>
      <li>Releasing <a href="https://github.com/google-research-datasets/wiki-atomic-edits">WikiAtomicEdits</a>: a data set of 43 million atomic edits across 8 languages, built from Wikipedia.</li>
      <li>Analysis of the data set brings the following conclusions: Inserted language differs from general Wikipedia content: Language models trained on edit data presents different aspects of semantics and discourse than models trained on raw, unstructured text.</li>
    </ul>
  </li>
</ul>

<p>Link to the <a href="https://arxiv.org/pdf/1808.09422.pdf">paper</a>.</p>

<h3 id="the-importance-of-scaling-down-one-weird-trick-to-make-your-nlp-projects-more-successful">The importance of scaling down: One weird trick to make your NLP projects more successful</h3>

<p>This presentation was delivered by Matthew Honnibal (Explosion AI).</p>

<ul>
  <li>How to maximize NLP project failure? Imagineer (Unrealistic use cases), Forecast, Outsource data collection (little knowledge about the project’s data requirements), wire (stacking layers with no context understanding), ship (delivering the project as is).</li>
  <li>Machine learning hierarchy of needs, in order of importance:
    <ul>
      <li>Understanding how the model will work in the larger application or business process: including tolerance for inaccuracies, latencies, etc.</li>
      <li>Annotation scheme and corpus construction: categories that will be easy to annotate consistently and easy for the model to learn.</li>
      <li>Consistent and clean data: attentive annotators, good quality control processes.</li>
      <li>Model architecture: smart choices, no bugs.</li>
      <li>Optimization: given by hyper-parameters, initialization tricks, etc.</li>
    </ul>
  </li>
  <li>Importance of iteration over data <span style="text-decoration:underline;">and</span> code:
    <ul>
      <li>Problem: It is easy to make modeling decisions that are simple, obvious and wrong. Solution: Compose generic models into novel solutions.</li>
      <li>Problem: Big annotation projects make evidence expensive to collect. Solution: Run your own micro-experiments.</li>
    </ul>
  </li>
  <li>A/B evaluation beats BLEU scores blues: don’t settle for proxy metrics and build micro A/B tests.</li>
</ul>

<h3 id="rapid-nlp-annotation-through-binary-decisions-pattern-bootstrapping-and-active-learning">Rapid NLP Annotation through Binary Decisions, Pattern Bootstrapping and Active Learning</h3>

<p>This presentation was delivered by Ines Montani (Explosion AI).</p>

<ul>
  <li>Feedback and best practices on annotation pipelines from the developers of <a href="https://prodi.gy">prodigy</a>.</li>
  <li>Annotation needs iteration: can’t expect to define the task correctly the first time.</li>
  <li>It has to be semi-automatic: boring tasks would never be performed reliably.</li>
  <li>Binary annotation is key for golden data sets: faster, more reliable and generalizable.</li>
  <li>Avoid cold starts: use simple models (e.g. rule based) and leverage active learning.</li>
</ul>

<p>More on the <a href="https://speakerdeck.com/inesmontani/belgium-nlp-meetup-rapid-nlp-annotation-through-binary-decisions-pattern-bootstrapping-and-active-learning">slides</a>.</p>

<h3 id="large-scale-fact-extraction-and-verification">Large-scale Fact Extraction and Verification</h3>

<p>This presentation was delivered by Arpit Mittal (Amazon), to explain the paper: FEVER: a large-scale dataset for Fact Extraction and VERification.</p>

<ul>
  <li>Release of <a href="http://fever.ai/">FEVER</a>: a data set for claim verification against textual sources. It consists of 185k+ claims extracted from Wikipedia, verified and annotated with 1 of 3 labels: supported, refuted, not enough info.</li>
  <li>Two objectives: Transform free-form text to structured information AND verify facts (in order to help combat fake news).</li>
</ul>

<p>Link to the <a href="https://arxiv.org/abs/1803.05355">paper</a>.</p>

<h3 id="transfer-learning-with-language-models">Transfer learning with language models</h3>

<p>This presentation was delivered by Sebastian Ruder (Aylien).</p>

<ul>
  <li>Recent Advances: ELMo, ULMFiT, OpenAI Transformer (12 layers, 8 GPUs, 1 month), and BERT (24 layers, 64 TPUs, 4 days / 8 GPUs, 40-70 days). SOTA on a wide range of tasks (cheaper use cases once LM trained).</li>
  <li>Tips for LM training: bidirectional architecture, choice of loss/auxiliary loss, etc.</li>
  <li>LMs capture: structure of the text, syntax, meter, hierarchical relation (e.g. coherence, syntax, POS tagging, constituents).</li>
  <li>No indication of performance ceiling: fine-tuning LMs will become commonplace in NLP.</li>
  <li>Future directions: True multilingual NLP, more challenging problems (NLU, common sense inference), better interpretation of LMs.</li>
</ul>

<p>For more details, check the <a href="https://drive.google.com/open?id=1kmNAwrSlFYo0cN_DcURMOArBwe9FxWxR">slides</a>.</p>

<h1 id="day-2-november-1st-2018">Day 2: November 1<sup>st</sup>, 2018</h1>

<h2 id="tutorial-4--deep-latent-variable-models-of-natural-language">Tutorial 4:  Deep Latent Variable Models of Natural Language</h2>

<p>The tutorial was presented by Yoon Kim, Sam Wiseman, and Alexander Rush from Harvard NLP Group.</p>

<ul>
  <li>Tractable inference over the latent variables: including neural extensions of tagging and parsing models.</li>
  <li>Non-tractable inference over the latent variables: restricted to continuous latent variables.</li>
  <li>Overview of recent developments in neural variational inference (e.g. auto-encoders), the challenges they undertake and the best practices.</li>
</ul>

<p>Link to the resources shared as part of the tutorial <a href="http://nlp.seas.harvard.edu/latent-nlp-tutorial.html">here</a>.</p>

<h2 id="workshop-2-conll">Workshop 2: CoNLL</h2>

<h3 id="invited-talk-semantic-spaces-across-diverse-languages">Invited talk: Semantic spaces across diverse languages</h3>

<p>Presentation by Asifa Majid (University of York).</p>

<ul>
  <li>Natural languages are NOT equally expressible: some concepts exist in one language but not in another, e.g. there are 421 words for snow in Scottish.</li>
  <li>Culture and language shape our internal representation of concepts / perception of the world:
    <ul>
      <li>If you ask speakers of different languages to color in different body parts in a picture, the body parts that are associated with each term depend on the language.</li>
      <li>When most languages lack terms describing specific scents and odors. In contrast, the <a href="https://en.wikipedia.org/wiki/Jahai_people">Jahai have</a> half a dozen terms for different qualities of smell.</li>
    </ul>
  </li>
  <li>Interesting to incorporate insights from psycholinguistics in how we model words across languages and different cultures, as cross-lingual embeddings have mostly focused on word-to-word alignment.</li>
</ul>

<p><strong>Comparing Models of Associative Meaning: An Empirical Investigation of Reference in Simple Language Games</strong></p>

<p>This <a href="https://arxiv.org/abs/1810.03717">paper</a> was written by Judy Hanwen Shen, Matthias Hofer, Bjarke Felbo and Roger Levy. It presents the nature of the lexical resources that speakers and listeners can bring to bear in achieving reference through associative meaning alone.</p>

<h3 id="sequence-classification-with-human-attention-special-paper-award">Sequence Classification with Human Attention (special paper award)</h3>

<p>This <a href="http://aclweb.org/anthology/K18-1030">paper</a> was written by Maria Barrett, Joachim Bingel, Nora Hollenstein, Marek Rei and Anders Søgaard. It highlights the fact that human attention provides a good inductive bias on many attention functions in NLP. They use estimated human attention, by eye tracking, corpora to regularize the attention in RNNs.</p>

<h2 id="tutorial-6-deep-chit-chat-deep-learning-for-chatbots">Tutorial 6: Deep Chit-Chat: Deep Learning for Chatbots</h2>

<p>This tutorial was presented by Wei Wu (Microsoft) and Rui Yan (Peking University).</p>

<ul>
  <li>Motivation:
    <ul>
      <li>Hot subject both in academia (e.g. the Alexa Prize) and industry (<em>Virtual Assistants</em>: MS Cortana, Apple Siri, Baidu Duer / <em>Smart speakers</em>: Amazon Alexa, Google Home / <em>Social Bot and customer service</em>)</li>
      <li>54% of conversations in chatbots are chit-chat (Study performed in Japan).</li>
      <li>Chit-chat is fundamentally different from goal oriented chatbots: open domain conversations should be relevant and diverse to keep the user engaged.</li>
      <li>Services (e.g. Recommendation, search, Q&amp;A) are connected via chat in chatbots.</li>
      <li>Task oriented vs Non-task oriented chatbots.</li>
      <li>Non task oriented chatbots are based on retrieval based methods or generation based methods.</li>
    </ul>
  </li>
  <li>Deep learning for NLP :
    <ul>
      <li>Word embedding: Word2Vec (CBOW, Skip-Gram), Glove, Fasttext.</li>
      <li>Sentence embedding.</li>
      <li>Application in dialogue modeling (seq2seq with attention).</li>
    </ul>
  </li>
  <li>Retrieval based chatbots:
    <ul>
      <li>Message-Response matching for single turn response selection.</li>
      <li>Context-Response matching for multi-turn response selection.</li>
      <li>Merging research directions: matching with better representations, matching with unlabeled data.</li>
    </ul>
  </li>
  <li>Generation based chatbots:
    <ul>
      <li>Single-turn generation: seq2seq, attention, bidirectional (MT inspiration).</li>
      <li>Multi-turn generation: Hierarchical context modeling.</li>
    </ul>
  </li>
  <li>Many other subjects include: diversity in conversations, content introducing, topics and emotion in conversations, persona in chat, reinforcement and adversarial learning in conversations.</li>
  <li>Evaluation metrics for conversations: Weak correlation between human and automatic evaluations (BLEU, information, e.g. entropy and perplexity, diversity, average response length, ADEM, RUBER).</li>
  <li>Future trends: Learning methods and representations, reasoning in dialogues (context based, knowledge, and common sense), X-grounded dialogues (labels, multimodal, texts), evaluation and benchmark data sets.</li>
</ul>

<h1 id="day-3-november-2nd-2018">Day 3: November 2<sup>nd</sup>, 2018</h1>

<h2 id="opening-remarks">Opening remarks</h2>

<p>Presentation of the conference key numbers, program and organization details.</p>

<ul>
  <li>EMNLP 2018 is HUGE:
    <ul>
      <li>2.1k+ submitted papers (+46% increase over 2017): 549 accepted papers: 24.6% acceptance rate.</li>
      <li>79 demo submissions (40% increase over 2017): 29 accepted demos (40% acceptance rate).</li>
      <li>2.5K attendees: &gt;100% increase over 2017.</li>
    </ul>
  </li>
  <li>EMNLP in numbers:
    <ul>
      <li>14 workshops, 6 tutorials, 351 long paper presentations, 198 short paper presentations, 10 TACL paper presentations, 29 demo presentations.</li>
      <li>60 area chairs, 1436 long/short papers reviewers, 150 demo paper reviewers.</li>
      <li>Gender diversity: 74.7% male reviewers, 68.3% male area/program chairs.</li>
      <li>4 plenaries: 3 keynotes + Best papers awards.</li>
      <li>11 parallel sessions: 4 oral sessions, 1 poster session.</li>
    </ul>
  </li>
  <li>Companies: Bloomberg, Google, Facebook, Salesforce, Apple, ASAPP, Amazon, Baidu, Grammarly, Naver Labs Europe, Ku Leuven, FWO, Megagon Labs, Huawei, eBay, Microsoft, Naver Line, Oracle, PolyAI, Sogou, YITI, Duolingo, Nuance, Shannon.ai, NextAI, textkernel, Allen Institute for AI, text IQ, etc.</li>
</ul>

<h2 id="keynote-i-truth-or-lie-spoken-indicators-of-deception-in-speech">Keynote I: Truth or Lie? Spoken Indicators of Deception in Speech</h2>

<p>This keynote was presented by Julia Hirschberg (Columbia University). The key ideas are:</p>

<ul>
  <li>Creation of a cross-cultural deception corpus: 340 subjects balanced by native language and gender.</li>
  <li>Hard to control potential indicators of deception.</li>
  <li>Features such as gender, ethnicity, culture and personality should be included in the models.</li>
  <li>Other features include: text-based, speech-based, syntactic features, personality features, etc.</li>
  <li>Classifiers: DNNS, BLSTMs, Hybrid methods.</li>
  <li>Conclusions include: Both humans and classifiers classify long answers as lies.</li>
  <li>The project is sponsored by the US government.</li>
</ul>

<p>Find more on the <a href="https://drive.google.com/file/d/17Ke40bHHnUyNrdA4twcyKI17D_B22Oim/view?usp=sharing">slides</a>.</p>

<h2 id="1a-social-applications-i">1A: Social Applications I</h2>

<ul>
  <li><em><span style="text-decoration:underline;">Privacy-preserving Neural Representations of Text</span></em>: This <a href="http://aclweb.org/anthology/D18-1001">paper</a> deals with adversarial privacy attacks on deep learning NLP systems. The idea is to attack hidden layers in a NN to get information about the input. The authors investigate the tradeoff between the utility and the privacy of the hidden representations and suggest defense methods based on the alteration of training objectives.</li>
  <li><em><span style="text-decoration:underline;">Adversarial Removal of Demographic Attributes from Text Data</span></em>: In this <a href="http://aclweb.org/anthology/D18-1002">paper</a>, the authors show that demographic information is encoded in intermediate representations and can alter classifiers’ decisions. They suggest removing these attributes using adversarial training, then retraining high level classifiers. They conclude that adversarial training is not enough to reach invariant representation of sensitive attributes.</li>
  <li><em><span style="text-decoration:underline;">DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning</span></em>: To detect fake news, the trend is to use external sources, which requires extensive feature modeling. This <a href="http://aclweb.org/anthology/D18-1003">paper</a> suggests a NN, which judiciously aggregates external knowledge and provides human readable explanations for the model’s results.</li>
  <li><em><span style="text-decoration:underline;">It’s going to be okay: Measuring Access to Support in Online Communities</span></em>: This <a href="http://aclweb.org/anthology/D18-1004">paper</a> analyzes the accessibility to online support, when revealing one’s gender. The authors present a data set and a method to assess accessibility to support on online platforms. Moreover, they suggest a strategy to infer gender from text and usernames.</li>
  <li><em><span style="text-decoration:underline;">Detecting Gang-Involved Escalation on Social Media Using Context</span></em>: This <a href="http://aclweb.org/anthology/D18-1005">paper</a> presents a method to detect expressions of violence and grief on social media. The authors use a domain specific unlabeled corpus and present a contextual and emotional representation of a users’ generated content. They made the collected data available upon request.</li>
</ul>

<h2 id="2c-multilingual-methods-i">2C: Multilingual Methods I</h2>

<ul>
  <li><em><span style="text-decoration:underline;">Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging</span></em>: The <a href="http://aclweb.org/anthology/D18-1061">paper</a> introduces a cross-lingual neural POS tagger, by leveraging annotation projection, instance selection, tag dictionaries, morphological lexicons and distributed representations, without access to any golden data.</li>
  <li><em><span style="text-decoration:underline;">Unsupervised Bilingual Lexicon Induction via Latent Variable Models</span></em>: Many works on bilingual lexicons extraction are based on aligning monolingual word embeddings. This <a href="http://aclweb.org/anthology/D18-1062">paper</a> suggests using latent variable models and adversarial training to do so. The method was tested on several language pairs.</li>
  <li><em><span style="text-decoration:underline;">Learning Unsupervised Word Translations without Adversaries</span></em>: The SOTA of unsupervised bilingual dictionaries induction use adversarial methods (thus suffer from instability and hyper parameters sensitivity). This <a href="http://aclweb.org/anthology/D18-1063">paper</a> presents a statistical method to unsupervised dictionaries extraction with adversarial training.</li>
  <li><em><span style="text-decoration:underline;">Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification</span></em>: The problem with joint models is that shared networks learn the majority data set features. To solve this, the <a href="http://aclweb.org/anthology/D18-1064">paper</a> suggests training language-specific task adversarial networks and task-specific language adversarial networks. The goal is to purge language specific and task specific dependencies. The method is tested on 2 languages and 3 tasks.</li>
  <li><em><span style="text-decoration:underline;">Surprisingly Easy Hard-Attention for Sequence to Sequence Learning</span></em>: This <a href="http://aclweb.org/anthology/D18-1065">paper</a> shows that beam approximation of joint distribution between attention and output is efficient for seq2seq learning. The method combines sharp focus in hard attention and implementation ease of soft attention.</li>
</ul>

<h2 id="3a-machine-translation-i">3A: Machine Translation I</h2>

<ul>
  <li><em><span style="text-decoration:underline;">SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation</span></em>: This <a href="http://aclweb.org/anthology/D18-1100">paper</a> presents a data augmentation technique for NMT. It consists of randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies.</li>
  <li><em><span style="text-decoration:underline;">Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder</span></em>: This <a href="http://aclweb.org/anthology/D18-1101">paper</a> presents a word by word translation strategy that uses cross-lingual word embeddings. For context encoding, they suggest using a language model and for word reordering, they use a denoising autoencoder. An advantage of this method is not using iterative approaches, such as back-translation.</li>
  <li><em><span style="text-decoration:underline;">Decipherment of Substitution Ciphers with Neural Language Models</span></em>: This <a href="http://aclweb.org/anthology/D18-1102">paper</a> proposes a “beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM”. The authors suggest augmenting “beam search with a novel rest cost estimation that exploits the prediction power of a neural LM”.</li>
  <li><em><span style="text-decoration:underline;">Rapid Adaptation of Neural Machine Translation to New Languages</span></em>: The main idea about this <a href="http://aclweb.org/anthology/D18-1103">paper</a> is using high resource languages to train what the authors call “seed models”, then fine-tuning on low resource languages. They suggest jointly training similar languages to avoid overfitting on LRLs. Findings include that multilingual models perform well on LRLs, even when the training data doesn’t present examples in these languages.</li>
  <li><em><span style="text-decoration:underline;">Compact Personalized Models for Neural Machine Translation</span></em>: This <a href="http://aclweb.org/anthology/D18-1104">paper</a> shows that it is possible to freeze the majority of the models parameters when adapting a translation model, with no reduction in performance. To ensure the sparsity of the tensors’ offsets, the authors suggest using group lasso regularization.</li>
</ul>

<h2 id="4a-language-models">4A: Language Models</h2>

<ul>
  <li><em><span style="text-decoration:underline;">Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</span></em>: This <a href="http://aclweb.org/anthology/D18-1149">paper</a> presents an iterative algorithm that considers any sequence generation as a latent variable model and refines it using a denoising approach. The algorithm was tested using the transformer model on two different tasks, machine translation and image caption generation.</li>
  <li><em><span style="text-decoration:underline;">Large Margin Neural Language Model</span></em>: In this <a href="http://aclweb.org/anthology/D18-1150">paper</a>, the authors suggest using a large margin criterion in LM training instead of perplexity minimization. The main idea is to enlarge the margin between good vs bad sequences in a task-specific sense.</li>
  <li><em><span style="text-decoration:underline;">Targeted Syntactic Evaluation of Language Models</span></em>: This is a data set <a href="http://aclweb.org/anthology/D18-1151">paper</a>. The authors present a benchmark data set to evaluate the “grammaticality” of the predictions of a LM. They conclude that there is considerable room for improvement in syntax learning using LMs.</li>
  <li><em><span style="text-decoration:underline;">Rational Recurrences</span></em>: This <a href="http://aclweb.org/anthology/D18-1152">paper</a> shows that RNNs have a connection to WFSAs, the same as CNNs (previously demonstrated). The authors try to transfer intuitions from classical models like WFSAs into neural network architectures and design.</li>
  <li><em><span style="text-decoration:underline;">Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling</span></em>: LMs are commonly used for various NLP tasks, due to the huge size of the available training data. However, large LMs present heavy computation constraints, during the inference. Thus, the <a href="http://aclweb.org/anthology/D18-1153">paper</a> presents an approach to LM compression, which keeps only useful information for a specific task.</li>
</ul>

<h1 id="day-4-november-3rd-2018">Day 4: November 3<sup>rd</sup>, 2018</h1>

<h2 id="5c-ir--text-mining">5C: IR / Text Mining</h2>

<ul>
  <li><em><span style="text-decoration:underline;">Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment</span></em>: This <a href="http://aclweb.org/anthology/D18-1209">paper</a> introduces a “word-by-word alignment  framework  that  measures  the  compatibility of embeddings between word pairs, and then  adaptively  accumulates  these  alignment features  with  a  simple  yet  effective  aggregation  function”.</li>
  <li><em><span style="text-decoration:underline;">Learning Context-Sensitive Convolutional Filters for Text Processing</span></em>: In this <a href="http://aclweb.org/anthology/D18-1210">paper</a>, the authors present an approach that uses a small meta network to learn context-aware convolutional filters for text processing. This network encodes the contextual information in “input-aware” filters.</li>
  <li><em><span style="text-decoration:underline;">Deep Relevance Ranking Using Enhanced Document-Query Interactions</span></em>: This <a href="http://aclweb.org/anthology/D18-1211">paper</a> presents a document relevance ranking model, which is obtained by augmenting DRMM with rich context encodings.</li>
  <li><em><span style="text-decoration:underline;">Learning Neural Representation for CLIR with Adversarial Framework</span></em>: This <a href="http://aclweb.org/anthology/D18-1212">paper</a> presents a new cross-lingual IR model, built using adversarial learning.</li>
  <li><em><span style="text-decoration:underline;">AD3: Attentive Deep Document Dater</span></em>: This <a href="http://aclweb.org/anthology/D18-1213">paper</a> presents an attention-based neural document  dating  system  which  utilizes  both  context  and  temporal  information  in  documents.</li>
</ul>

<h2 id="6d-multilingual-methods-ii">6D: Multilingual Methods II</h2>

<ul>
  <li><em><span style="text-decoration:underline;">Sentence Compression for Arbitrary Languages via Multilingual Pivoting</span></em>: This <a href="http://aclweb.org/anthology/D18-1267">paper</a> leverages bilingual corpora to perform sentence compression. The method consists of translating a source string into a foreign language and then back-translating it into the source language while controlling the translation length.</li>
  <li><em><span style="text-decoration:underline;">Unsupervised Cross-lingual Transfer of Word Embedding Spaces</span></em>: This <a href="http://aclweb.org/anthology/D18-1268">paper</a> deals with learning mapping functions between embedding spaces of different languages. It proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Instead, the authors suggest optimizing the transformation functions in both directions simultaneously based on distributional matching and the minimization of the back-translation losses.</li>
  <li><em><span style="text-decoration:underline;">XNLI: Evaluating Cross-lingual Sentence Representations</span></em>: This is a data set <a href="http://aclweb.org/anthology/D18-1269">paper</a>: The authors open source a data set for NLI in 15 languages and present different baselines to perform cross-lingual NLI.</li>
  <li><em><span style="text-decoration:underline;">Joint Multilingual Supervision for Cross-lingual Entity Linking</span></em>: This <a href="http://aclweb.org/anthology/D18-1270">paper</a> shows the limitations of XEL and the added value due to multilingual joint learning in low resource settings.</li>
  <li><em><span style="text-decoration:underline;">Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition</span></em>: This <a href="http://aclweb.org/anthology/D18-1271">paper</a> presents a decipherment algorithm with diverse clues to decipher the networks for accurate text stream alignment.</li>
</ul>

<h2 id="7a-dialogue-ii">7A: Dialogue II</h2>

<ul>
  <li><em><span style="text-decoration:underline;">Session-level Language Modeling for Conversational Speech</span></em>: This <a href="http://aclweb.org/anthology/D18-1296">paper</a> generalizes language models for  conversational  speech  recognition  to  allow them to operate across utterance  boundaries and speaker changes,  thereby capturing conversation-level  phenomena  such  as  adjacency  pairs,  lexical  entrainment,  and  topical coherence.</li>
  <li><em><span style="text-decoration:underline;">Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method</span></em>: Seq2seq generation models tend to generate generic/dull responses. Thus, the authors of this <a href="http://aclweb.org/anthology/D18-1297">paper</a> propose a statistical re-weighting method that assigns different weights for the multiple responses of the same query, and trains the standard neural generation model with the weights.</li>
  <li><em><span style="text-decoration:underline;">Training Millions of Personalized Dialogue Agents</span></em>: This <a href="http://aclweb.org/anthology/D18-1298">paper</a> suggests increasing the engagement level in conversational systems by making them more personalized. The authors open source a dataset of 5 million personas and 700 million persona-based dialogues and show that using personas improves the performance on end-to-end systems.</li>
  <li><em><span style="text-decoration:underline;">Towards Universal Dialogue State Tracking</span></em>: The <a href="http://aclweb.org/anthology/D18-1299">paper</a> introduces “StateNet”,  a universal dialogue state tracker: It is independent of the number of values, shares parameters across all slots,  and  uses  pre-trained  word  vectors  instead  of  explicit  semantic  dictionaries.</li>
  <li><em><span style="text-decoration:underline;">Semantic Parsing for Task Oriented Dialog using Hierarchical Representations</span></em>: Dialog systems usually deal with one query (intent) at a time. In this <a href="http://aclweb.org/anthology/D18-1300">paper</a>, the authors suggest a hierarchical annotation scheme for semantic parsing that allows the representation of compositional queries. Furthermore, they release a dataset of 44k annotated queries.</li>
</ul>

<h2 id="keynote-ii-understanding-the-news-that-moves-markets">Keynote II: Understanding the News that Moves Markets</h2>

<p>This keynote was presented by Gideon Mann (Bloomberg). The key ideas are:</p>

<ul>
  <li>The talk deals with financial technology, the news that moves the markets, computer things, etc.</li>
  <li>The importance of real time NLP on news data to understand and influence markets: speed and precision requirements.</li>
  <li>News generation on-demand.</li>
</ul>

<p>Find more on the <a href="https://drive.google.com/file/d/183r_3X3yzDhEWM43e2pOoS_w3kUxJbaR/view?usp=sharing">slides</a>.</p>

<h2 id="8a-text-categorization">8A: Text Categorization</h2>

<ul>
  <li><em><span style="text-decoration:underline;">Zero-shot User Intent Detection via Capsule Neural Networks</span></em>: This <a href="http://aclweb.org/anthology/D18-1348">paper</a> aims to solve the annotation problem in intents’ detection, in order to work on intents where no labeled utterances are available. Thus, the authors propose two   capsule-based   architectures: INTENT-CAPSNET (extracts semantic features from utterances and aggregates them to discriminate existing  intents)  and  INTENTCAPSNET-ZSL (gives  INTENT-CAPSNET the  zero-shot learning  ability  to  discriminate  emerging  intents via knowledge transfer from existing intents).</li>
  <li><em><span style="text-decoration:underline;">Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts</span></em>: This <a href="http://aclweb.org/anthology/D18-1349">paper</a> suggests a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences and help classify the current sentence.</li>
  <li><em><span style="text-decoration:underline;">Investigating Capsule Networks with Dynamic Routing for Text Classification</span></em>: This <a href="http://aclweb.org/anthology/D18-1350">paper</a> shows that capsule networks exhibit significant improvement when transferring single-label  to  multi-label text classification over the competitors.</li>
  <li><em><span style="text-decoration:underline;">Topic Memory Networks for Short Text Classification</span></em>: This <a href="http://aclweb.org/anthology/D18-1351">paper</a> proposes topic memory networks for short text classification with a novel topic memory mechanism to encode latent topic representations indicative of class labels.</li>
  <li><em><span style="text-decoration:underline;">Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces</span></em>: The authors of this <a href="http://aclweb.org/anthology/D18-1352">paper</a> perform a fine-grained evaluation to explain how state-of-the-art methods perform on infrequent labels.  They also develop few- and zero-shot methods for multi-label text classification when there is a known structure over the label space.</li>
</ul>

<h1 id="day-5-november-4th-2018">Day 5: November 4<sup>th</sup>, 2018</h1>

<h2 id="9b-sentiment-i">9B: Sentiment I</h2>

<ul>
  <li><em><span style="text-decoration:underline;">Sentiment Classification towards Question-Answering with Hierarchical Matching Network</span></em>: This <a href="http://aclweb.org/anthology/D18-1401">paper</a> proposes a novel task/method to address QA sentiment analysis. The authors create a high-quality annotated corpus with specially-designed annotation guidelines for QA-style sentiment classification.</li>
  <li><em><span style="text-decoration:underline;">Cross-topic Argument Mining from Heterogeneous Sources</span></em>: The authors of this <a href="http://aclweb.org/anthology/D18-1402">paper</a> propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts. They also open source annotations for over 25k instances covering 8 controversial topics.</li>
  <li><em><span style="text-decoration:underline;">Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised</span></em>: This <a href="http://aclweb.org/anthology/D18-1403">paper</a> combines two weakly supervised components to identify salient opinions and form extractive summaries from multiple reviews.</li>
  <li><em><span style="text-decoration:underline;">CARER: Contextualized Affect Representations for Emotion Recognition</span></em>: This <a href="http://aclweb.org/anthology/D18-1404">paper</a> proposes a semi-supervised, graph-based algorithm to produce rich structural descriptors which serve as the building blocks for constructing contextualized affect representations from text.</li>
  <li><em><span style="text-decoration:underline;">[TACL] Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification</span></em>: This <a href="http://emnlp2018.org/downloads/tacl-papers/EMNLP-TACL04.pdf">paper</a> proposes an Adversarial Deep Averaging Network (ADAN) to transfer the knowledge learned from labeled data on a resource-rich source language to low-resource languages where only unlabeled data exists.</li>
</ul>

<h2 id="10a-question-answering-iii">10A: Question Answering III</h2>

<ul>
  <li><em><span style="text-decoration:underline;">Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings</span></em>: The authors of this <a href="http://aclweb.org/anthology/D18-1452">paper</a> address  jointly  two  tasks for Question Answering  in  community  forums: given  a  new  question, find  related  existing  questions  AND find  relevant  answers to this new question.</li>
  <li><em><span style="text-decoration:underline;">What Makes Reading Comprehension Questions Easier</span></em>: The authors of this <a href="http://aclweb.org/anthology/D18-1453">paper</a> investigate what makes questions easier across recent 12  MRC  datasets  with  three  question  styles (answer  extraction,   description,   and  multiple  choice).</li>
  <li><em><span style="text-decoration:underline;">Commonsense for Generative Multi-Hop Question Answering Tasks</span></em>: The authors of this <a href="http://aclweb.org/anthology/D18-1454">paper </a> focus on a challenging multi-hop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context  to  generate  an  answer.</li>
  <li><em><span style="text-decoration:underline;">Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text</span></em>: The authors of this <a href="http://aclweb.org/anthology/D18-1455">paper</a> investigate QA over the combination of a KB and entity-linked text, which is appropriate when an incomplete KB is available with a large text corpus.</li>
  <li><em><span style="text-decoration:underline;">A Nil-Aware Answer Extraction Framework for Question Answering</span></em>: In this <a href="http://aclweb.org/anthology/D18-1456">paper</a>, the authors focus on developing QA systems that can extract an answer for a question if and only if the associated passage contains an answer.</li>
</ul>

<h2 id="keynote-iii-the-moment-of-meaning-and-the-future-of-computational-semantics">Keynote III: The Moment of Meaning and the Future of Computational Semantics</h2>

<p>This keynote was presented by Johan Bos (University of Groningen). The key ideas are:</p>

<ul>
  <li>Motivations include: Future language technology requires semantic interpretation (Explainable NLP) and better MT evaluation.</li>
  <li>Integrating lexical with formal semantics, language neutral semantic annotation and multilingual models.</li>
  <li>Future directions for semantics:
    <ul>
      <li>Computational semantics: more resources for inference, explainable NLP and thinking more “multilingual”.</li>
      <li>Adding meaning to MT: outperform BLEU and verify translation with semantic parsing.</li>
    </ul>
  </li>
</ul>

<p>Find more on the <a href="https://drive.google.com/file/d/1xr8bnf1VnUZP9Ew1h8fjKCEs5InHhNps/view?usp=sharing">slides</a>.</p>

<h2 id="best-paper-awards">Best Paper Awards</h2>

<ul>
  <li>Best Long papers:
    <ul>
      <li><em><span style="text-decoration:underline;">Linguistically-Informed Self-Attention for Semantic Role Labeling</span></em>: In this <a href="http://aclweb.org/anthology/D18-1548">paper</a>, the authors present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-of-speech tagging, predicate detection and SRL.</li>
      <li><em><span style="text-decoration:underline;">Phrase-Based &amp; Neural Unsupervised Machine Translation</span></em>: This <a href="http://aclweb.org/anthology/D18-1549">paper</a> investigates how to learn to translate when having access to only large monolingual corpora in each language. The authors propose two model variants, a neural and a phrase-based model.</li>
    </ul>
  </li>
  <li>Best short paper:
    <ul>
      <li><em><span style="text-decoration:underline;">How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks</span></em>: This <a href="http://aclweb.org/anthology/D18-1546">paper</a> gives baselines for  the  bAbI,  SQuAD,  CBT,  CNN,  and  Who-did-What datasets,  finding that question- and passage-only  models  often  perform  surprisingly  well.</li>
    </ul>
  </li>
  <li>Best resource paper:
    <ul>
      <li><em><span style="text-decoration:underline;">MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling</span></em>: This <a href="http://aclweb.org/anthology/D18-1547">paper</a> introduces the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.</li>
    </ul>
  </li>
</ul>

<h1 id="other-resources">Other resources</h1>

<ul>
  <li><a href="https://blackboxnlp.github.io/">Link</a> to the “Black Box NLP” tutorial.</li>
  <li><a href="https://github.com/allenai/writing-code-for-nlp-research-emnlp2018">Link</a> to the “Writing Code for NLP Research” tutorial.</li>
</ul>

<p><em>Blog posts:</em></p>
<ul>
  <li>Sebastian Ruder: <a href="http://ruder.io/emnlp-2018-highlights/">EMNLP 2018 Highlights: Inductive bias, cross-lingual learning, and more.</a></li>
  <li>Patrick Lewis: <a href="https://www.patricklewis.io/post/emnlp2018/">EMNLP 2018</a>.</li>
</ul>

<p><small><i>This article was shared privately on 2018-12-05 and publicly on 2024-05-19.</i></small></p>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                        <section class="author-card">
                            
                                <img class="author-profile-image" src="/assets/images/taycir.jpg" alt="tayciryahmed" />
                            
                            <section class="author-card-content">
                                <h4 class="author-card-name"><a href="/author/tayciryahmed">Taycir Yahmed</a></h4>
                                
                                    <p>Software Engineer, Maching Learning</p>
                                
                            </section>
                        </section>
                        <div class="post-full-footer-right">
                            <a class="author-card-button" href="/author/tayciryahmed">Read More</a>
                        </div>
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/mcQA">
                <div class="post-card-image" style="background-image: url(/assets/images/writing.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/mcQA">
                <header class="post-card-header">
                    

                    <h2 class="post-card-title">mcQA - Multiple Choice Question Answering</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p></p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/images/taycir.jpg" alt="Taycir Yahmed" />
                        
                        <span class="post-card-author">
                            <a href="/author/tayciryahmed/">Taycir Yahmed</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      1 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/Building-Parallel-Corpora-CLBOW">
                <div class="post-card-image" style="background-image: url(/assets/images/tags.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/Building-Parallel-Corpora-CLBOW">
                <header class="post-card-header">
                    

                    <h2 class="post-card-title">Building Parallel Corpora Using Cross-Lingual BOW</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p></p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/images/taycir.jpg" alt="Taycir Yahmed" />
                        
                        <span class="post-card-author">
                            <a href="/author/tayciryahmed/">Taycir Yahmed</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      1 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
                <img src="/assets/images/favicon.png" alt="Taycir Yahmed icon" />
            
            <span>Taycir Yahmed</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">EMNLP 2018 Highlights</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=EMNLP+2018+Highlights&amp;url=https://tayciryahmed.github.ioemnlp2018"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://tayciryahmed.github.ioemnlp2018"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Taycir Yahmed</a> &copy; 2025 Opinions are my own.</section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    <a href="https://github.com/tayciryahmed" target="_blank" rel="noopener">GitHub</a>
                    <a href="https://twitter.com/TaycirYahmed" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://www.linkedin.com/in/taycir" target="_blank" rel="noopener">LinkedIn</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-65JD9XK4QH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-65JD9XK4QH');
</script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
